# ETL-процесс. RestAPI.

## Описание проекта
Целью проекта является создать ETL процесс для доставки данных из разных источников в одно хранилище. Основным требованием для раелизации является создание такой системы, которая будет автономна, масштабируема, что бы ее компоненты не были напрямую зависимы друг от друга и вмешательство пользователей было минимально. Для предоставления данных поулченных с помощью ETL процесса реализовано RestAPI с системой доступа по API-ключам. Хранение данных о заболеваемости происходит в базе даннхы ClickHouse. Для хранения данных о пользователях и метаданных для AirFlow используется PostgreSQL. Каждый компонент системы находится в docker контейнере.

Вся система выглядит следующим образом:
<img width="1392" alt="image" src="https://github.com/user-attachments/assets/ad684ea0-3195-4dfb-85eb-dc50bb9a2f86" />


### ETL-процесс:

Для выполнения скриптов, написанных для извлечения, преобразования и выгрузки данных испрользуется AirFlow.

Общие принципы:

* Старые данные преобразовываются с помощью библиотки Pandas.
* Ветвление на 2 пайплайна: загрузка старых данных (если еще не загружены) и загрузка новых данных.
* Все данные после обработки отправляются в сериализации JSON в топик Kafka, из которого с помощью Kafka sink connector читаются и отправляются в базу данных ClickHouse

Реализованы следующие группы тасков:
- Branching таски (```BranchPythonOperator```)
    Проверяется, загружены ли старые данные. Если они уже загруженных - процесс начинает проверять на наличие новых данных на веб странице. Проверка на наличие новых данных и старых производится с помощью переменных среды выполнения AirFlow.

- transform таски (```PythonOperator```)
    Для старых данных используется библиотека Pandas. Пропущенные значения либо заполняются прошлым не пустым зачением. Данные после 15.05.2023 перестали поставлятся ежедневно - вместо этого, за неделю, я принял решение просто делить эти значения на 7 и заполнять этими значениями данные по дням за прошлую неделю. Для парсинга новых данных из веб страницы используется BeautifulSoup.

- ```load_data``` (```PythonOperator```)
    Загрузка всех данных из предыдущего таска, используется один скрипт для любых данных и осуществляется через брокер Kafka и Kafka clickhouse sink connector.


Целый граф задач выглядит следующим образом:
<img width="944" alt="image" src="https://github.com/user-attachments/assets/83d2f3eb-be30-48b5-83d2-c52c8195a303" />


### RestAPI

Приложение написано на фреймворке FastAPI. Используется трехслойная архитекутра - контроллеры, сервисы и слой доступа к данных (репозитории).

Слой доступа к данных состоит из двух компонентов:

```UserRepository``` и ```CovidDataRepository```, используют SQLAlchemy с postgresql asyncpg DBAPI и асинхронный clickhouse client соответсвенно.

Слой сервисов также состоит из двух компонентов по той же логике:
```UserService``` - логика работы с пользователями, создание пользователя, проверка пароля, токена и повторная выдача токена.
```CodivDataRepository``` - работа непостредственно с данными о заболеваемости COVID

Контроллеры:
```UserController``` - регистрация пользователей, повторная выдача апи ключа.
```CovidDataController``` - Доступ к данным. Используется аунтификация по API ключу в заголовку *x-key*.


Все компоненты используют Dependency Injection. Жизненный цикл сессии SQLAlchemy и клиента ClickHouse управляются внутри жизненного цикла приложения FastAPI с помощью встроенного механизма ```lifespan```.

После запуска, документация будет доступна на http://localhost:8000/docs#/ или http://localhost:8000/redoc
